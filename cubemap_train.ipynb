{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efeb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import scipy.io as io\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import open3d as o3d\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils.utils import getWriterPath\n",
    "from settings import EXPER_PATH\n",
    "from utils.loader import dataLoader, modelLoader, pretrainedLoader\n",
    "from utils.logging import *\n",
    "from copy import deepcopy as dc\n",
    "from utils.d2s import DepthToSpace, SpaceToDepth\n",
    "from train_cubemap import *\n",
    "from utils.utils import flattenDetection\n",
    "from Train_model_frontend_cubemap import thd_img\n",
    "from utils_custom import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "def measure_t(start, before):\n",
    "    now = time.time()\n",
    "    runt = now-start\n",
    "    h = runt//3600\n",
    "    m = (runt - h*3600)//60\n",
    "    s = round(runt - h*3600 - m*60)\n",
    "    prtstr = f\"iter {_iter+1} in {len(train_loader)}, \\\n",
    "            {round((_iter+1)/len(train_loader), 4) * 100}%\\\n",
    "            {h}h {m}m {s}s\"\n",
    "    runt2 = before-start\n",
    "    h2 = runt2//3600\n",
    "    m2 = (runt2 - h2*3600)//60\n",
    "    s2 = round(runt2 - h2*3600 - m2*60)\n",
    "    prtstr += f\"  ({h2}h {m2}m {s2}s)\"\n",
    "    return now  \n",
    "args = Namespace(command='train_joint', config='configs/magicpoint_cubemap.yaml', debug=False, eval=False, exper_name='cubemap_dataset', func=train_joint)\n",
    "\n",
    "\n",
    "from utils_custom import *\n",
    "from utils_custom_visualize import *\n",
    "\n",
    "with open(args.config, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "EXPER_PATH = 'logs'\n",
    "output_dir = EXPER_PATH\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c018a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO train on device: cuda\n",
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO => will save everything to logs/checkpoints\n",
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO workers_train: 1, workers_val: 1\n",
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO == train split size 2160 in 2160 batches\n",
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO == val split size 2160 in 2160 batches\n",
      "2023-04-10 16:23:18 koala-A520M-H root[25190] INFO => creating model: SuperPointNet_cubemap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: CubemapDataset\n",
      "not_warped_images:  True\n",
      "not_warped_images:  True\n",
      "Load Train_model_frontend!!\n",
      "check config!! {'train_iter': 200000, 'save_interval': 2000, 'tensorboard_interval': 1000, 'model': {'subpixel': {'enable': False}, 'train_only_descriptor': True, 'name': 'SuperPointNet_cubemap', 'params': {}, 'detector_loss': {'loss_type': 'softmax'}, 'batch_size': 1, 'eval_batch_size': 1, 'learning_rate': 0.001, 'kernel_reg': 0.0, 'detection_threshold': 0.001, 'nms': 4, 'dense_loss': {'enable': False, 'params': {'descriptor_dist': 4, 'lambda_d': 800}}, 'sparse_loss': {'enable': True, 'params': {'num_matching_attempts': 1000, 'num_masked_non_matches_per_match': 100, 'lamda_d': 1, 'dist': 'cos', 'method': '2d'}}, 'other_settings': 'train 2d, gauss 0.5'}, 'data': {'dataset': 'CubemapDataset', 'primitives': 'all', 'not_warped_images': True, 'preprocessing': {'resize': [1024, 1024]}}, 'front_end_model': 'Train_model_frontend_cubemap', 'retrain': False, 'reset_iter': True, 'validation_interval': 1000, 'validation_size': 10, 'train_show_interval': 1000, 'seed': 0, 'pretrained': 'logs/magicpoint_synth_t2/checkpoints/3channel.pth.tar'}\n",
      "HOMONUM: 100   HOMOBATCH: 3\n",
      "use sparse_loss!\n",
      "set train loader\n",
      "set train loader\n",
      "model:  SuperPointNet_cubemap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 16:23:19 koala-A520M-H root[25190] INFO => setting adam solver\n",
      "2023-04-10 16:23:19 koala-A520M-H root[25190] INFO reset iterations to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam optimizer\n",
      "load pretrained model from: %s logs/magicpoint_synth_t2/checkpoints/3channel.pth.tar\n",
      "successfully load pretrained model from: %s logs/magicpoint_synth_t2/checkpoints/3channel.pth.tar\n",
      "\n",
      "\n",
      " Train only descriptor\n",
      "=== Let's use 1 GPUs!\n",
      "adam optimizer\n"
     ]
    }
   ],
   "source": [
    "############################################### train_joint\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "task = config['data']['dataset']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info('train on device: %s', device)\n",
    "with open(os.path.join(output_dir, 'config.yml'), 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "# writer = SummaryWriter(getWriterPath(task=args.command, date=True))\n",
    "writer = SummaryWriter(getWriterPath(task=args.command, \n",
    "    exper_name=args.exper_name, date=True))\n",
    "## save data\n",
    "save_path = get_save_path(output_dir)\n",
    "\n",
    "############################################### \n",
    "data = dataLoader(config, dataset=task, warp_input=True)\n",
    "train_loader, val_loader = data['train_loader'], data['val_loader']\n",
    "\n",
    "datasize(train_loader, config, tag='train')\n",
    "datasize(val_loader, config, tag='val')\n",
    "# init the training agent using config file\n",
    "# from train_model_frontend import Train_model_frontend\n",
    "from utils.loader import get_module\n",
    "train_model_frontend = get_module('', config['front_end_model'])\n",
    "train_agent = train_model_frontend(config, save_path=save_path, device=device)\n",
    "\n",
    "# writer from tensorboard\n",
    "train_agent.writer = writer\n",
    "\n",
    "# feed the data into the agent\n",
    "train_agent.train_loader = train_loader\n",
    "train_agent.val_loader = val_loader\n",
    "\n",
    "# load model initiates the model and load the pretrained model (if any)\n",
    "train_agent.loadModel()\n",
    "train_agent.dataParallel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a13e45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(2.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1])[0]\n",
    "b = torch.Tensor([2])[0]\n",
    "print(a,b)\n",
    "for i in range(2):\n",
    "    b += a\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4656a020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor(3.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ca6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e20e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bb99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51817912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6e7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a237e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb43595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074f9335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample =next(iter(train_loader))\n",
    "net = dc(train_agent.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a15b108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['image', 'warped_image', 'ply_path', 'R', 'T', 'R_w', 'T_w', 'img_path', 'img_path_w', 'kpts2D', 'kpts3D', 'kpts2D_w', 'kpts3D_w']),\n",
       " torch.Size([1, 3, 1024, 1024]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys(), sample['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78468472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img, img_w = sample['image'].to(device), sample['warped_image'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfaeaf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape = (3, 4, 4)\n",
    "labels_2D = torch.Tensor([[0,0], [1,1], [3,1], [2,1]])\n",
    "background = torch.zeros(*img_shape).to(device)\n",
    "background[:, labels_2D[:,0].int(), labels_2D[:,1].int()] = 1\n",
    "background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "ae54ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samim = torch.Tensor(np.array(Image.open(imname2).resize((1024,1024))))\n",
    "samim = samim.transpose(1,2).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "598fabde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 1],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm = torch.zeros(3, 1, 4, 4)\n",
    "hm[0,0, 1,1] = 1\n",
    "hm[1,0, 1,2] = 1\n",
    "hm[2,0, 3,3] = 1\n",
    "rs, cs = torch.where(hm.reshape(12, 4) > 0)\n",
    "kpts = torch.stack((\n",
    "    cs, rs), dim=1)\n",
    "kpts = kpts.tolist()\n",
    "b_kpts = [[]]*3\n",
    "b_kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad021ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81619d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.reshape(12, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253cc7a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1582e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HOMO_NUM, HOMO_BATCH = 100, 20\n",
    "batch_size = img.shape[0]\n",
    "processed_img_names = []\n",
    "import time\n",
    "thd=0.2\n",
    "fnc = 1023.5\n",
    "camera_matrix = torch.Tensor([[fnc,0,fnc],[0,fnc,fnc],[0,0,1]])\n",
    "\n",
    "kpts2d_total, kpts2d_total_w = [[]]*batch_size, [[]]*batch_size\n",
    "kpts3d_total, kpts3d_total_w = [[]]*batch_size, [[]]*batch_size\n",
    "start = time.time()\n",
    "\n",
    "ptcloud_list = []\n",
    "for b in range(batch_size):\n",
    "    target = o3d.io.read_point_cloud(sample['ply_path'][b])\n",
    "    points = torch.Tensor(np.array(target.points)).to(device)\n",
    "    del target\n",
    "    ptcloud_list.append(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f99230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a42485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 내적값이 큰 kpt의 descriptor를 연결\n",
    "out['desc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "desc = train_agent.interpolate_to_dense(out['desc'])\n",
    "desc_w = train_agent.interpolate_to_dense(out_w['desc'])\n",
    "thd = 0.2\n",
    "# hms = thd_img(flattenDetection(out['semi']), thd=thd)\n",
    "# hms_w = thd_img(flattenDetection(out_w['semi']), thd=thd)\n",
    "hms = flattenDetection(out['semi'])\n",
    "hms_w = flattenDetection(out_w['semi'])\n",
    "print(desc.shape)\n",
    "b = 0\n",
    "kpts = get_kpts_from_hm(hms[b])\n",
    "kpts_w = get_kpts_from_hm(hms_w[b])\n",
    "print(\"Num of kpts: \", kpts.shape, kpts_w.shape)\n",
    "\n",
    "kpts_desc = get_desc_of_kpts(desc[b], kpts)\n",
    "kpts_desc_w = get_desc_of_kpts(desc_w[b], kpts_w)\n",
    "kpts_desc = kpts_desc.transpose(0, 1)\n",
    "kpts_desc_w = kpts_desc_w.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts_desc.shape, kpts_desc_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afd026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f530526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c01c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e68ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be78d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623ed8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.Tensor([[0,0],   [0,1],   [0,2],   [0,3],   [0,4]])\n",
    "# b = torch.Tensor([[0,0.2], [0,3.2], [0,3.1], [0,1.1], [0,4.1]])\n",
    "# '''\n",
    "# GT: 0 2 3 nan 4\n",
    "# Return: [0,1,3,4], [0,3,2,4]\n",
    "# '''\n",
    "\n",
    "# # a = torch.Tensor([[0,5],   [0,1],   [0,6],   [0,3],   [0,4]])\n",
    "# # b = torch.Tensor([[0,4], [0,5.51], [0,3.1], [0,1.1], [0,4.1]])\n",
    "# '''\n",
    "# Return: [1,2, 3, 4], [3,1,2,0]\n",
    "# '''\n",
    "\n",
    "# sort_by_nearest_pts(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbae854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_by_nearest_pts(kpts1, kpts2):\n",
    "    # sort kpts2 correspondent to kpts1\n",
    "    ary = torch.cdist(kpts2, kpts1)\n",
    "    mask1 = ary == torch.unsqueeze(torch.min(ary,axis=1).values, dim=1)\n",
    "    mask2 = ary == torch.min(ary, axis=0).values\n",
    "    idx1, idx2 = torch.where(mask1*mask2*(ary<10))\n",
    "    return kpts1[idx2], kpts2[idx1], ary[idx1, idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpts1, kpts2 = kpts.float(), kpts_w.float()\n",
    "# ary = torch.cdist(kpts2, kpts1)\n",
    "# mask1 = ary == torch.unsqueeze(torch.min(ary,axis=1).values, dim=1)\n",
    "# mask2 = ary == torch.min(ary, axis=0).values\n",
    "# idx1, idx2 = torch.where(mask1*mask2*(ary<5))\n",
    "# ary[idx1, idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16275287",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts_sorted, kpts_w_sorted, color = sort_by_nearest_pts(kpts.float(), kpts_w.float())\n",
    "kpts_w_sorted.shape, max(color), min(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 255-color*10\n",
    "color = torch.clamp(color, 0, 255)\n",
    "color = torch.stack([torch.zeros_like(color), color, color], axis=1)\n",
    "color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa905580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfig = make_matching_plot_fast(sample['img_path'][0], sample['img_path_w'][0], \n",
    "        kpts.detach().cpu().numpy(), kpts_w.detach().cpu().numpy(), \n",
    "        kpts_sorted.detach().cpu().numpy(), kpts_w_sorted.detach().cpu().numpy(), color,\n",
    "                                 img_shape=img.shape[2:])\n",
    "plt.imshow(outfig)\n",
    "plt.savefig('rd_glue2.jpg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3559c121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9643, 0.0704, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.1580, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.2231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.2669, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.2906, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.2951, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.2881, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 0.3396, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1,8,8)\n",
    "Hinfo = {'theta': 22.903534447864008, 't_x': -0.984204265504758, 't_y': 0.32934251819071, 'scale': 0.7376072030495775, 'startpoints': [[1.369664350014332, -1.067612001895284], [-1.088877579074605, 1.8467378868434], [7.2300664433097, 7.6867897737297], [7.757960087271, 7.527099415175414]], 'endpoints': [[   0.,    0.],\n",
    "       [   0., 8.],\n",
    "       [8., 8.],\n",
    "       [8.,    0.]]}\n",
    "a = apply_H_from_info(a, Hinfo)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e3d27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'NoneType'>\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(type(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebd31b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0, 2.0], [2.0, 6.0], [6.0, 6.0], [6.0, 2.0]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmedge(p4list, px):\n",
    "    new = []\n",
    "    for p in p4list:\n",
    "        e1, e2 = -px, -px\n",
    "        if p[0] == 0:\n",
    "            e1 *= -1\n",
    "        if p[1] == 0:\n",
    "            e2 *= -1\n",
    "        new.append([p[0]+e1, p[1]+e2])\n",
    "    return new\n",
    "\n",
    "rmedge(Hinfo['endpoints'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "thd=0.4\n",
    "out = net(img)\n",
    "hms = thd_img(flattenDetection(out['semi']), thd=thd)\n",
    "b=0\n",
    "hm = hms[b][0]\n",
    "rs, cs = torch.where(hm > 0)\n",
    "kpts = torch.stack((cs, rs), dim=1)\n",
    "print(len(kpts))\n",
    "showkpts(img[b].transpose(0,1).transpose(1,2).detach().cpu(), kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = np.unique(kpts_total[0], axis=0)\n",
    "uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thd=0.4\n",
    "img, img_w = sample['image'].to(device), sample['warped_image'].to(device)\n",
    "\n",
    "kpts_total = [[]]*2\n",
    "for homo_iter in range(3):\n",
    "    im_transformed, Hinv_info = apply_random_H_batch(img)\n",
    "    im_transformed_w, Hinv_info_w = apply_random_H_batch(img_w)\n",
    "    out = net(im_transformed)\n",
    "    out_w = net(im_transformed_w)\n",
    "\n",
    "    hms = flattenDetection(out['semi'])\n",
    "    im_orig = apply_H_from_info(hms, Hinv_info)\n",
    "    im_orig = thd_img(im_orig, thd=thd)\n",
    "\n",
    "    hms_w = flattenDetection(out_w['semi'])\n",
    "    im_orig_w = apply_H_from_info(hms_w, Hinv_info_w)\n",
    "    im_orig_w = thd_img(im_orig_w, thd=thd)\n",
    "\n",
    "    print(torch.sum(im_orig[0]), im_orig.shape)\n",
    "\n",
    "    for b in range(2):\n",
    "        hm, hm_w = im_orig[b][0], hms_w[b][0]\n",
    "        rs, cs = torch.where(hm > 0)\n",
    "        rs_w, cs_w = torch.where(hm_w > 0)\n",
    "        kpts = torch.stack((cs, rs), dim=1)\n",
    "        kpts_total[b]+=kpts.detach().cpu().tolist()\n",
    "b=0\n",
    "kpttotal = np.unique(kpts_total[0], axis=0)\n",
    "print(kpttotal.shape)\n",
    "showkpts(img[b].transpose(0,1).transpose(1,2).detach().cpu(), kpttotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aedd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_orig[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25431860",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(torch.flatten(torch.where(im_orig[0,:,:]>0.2, diff, 0)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68541e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97684d5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inputim = dc(imsam[:1])\n",
    "hms = flattenDetection(out['semi'])\n",
    "inputim = dc(hms[0])\n",
    "thd = 0.2\n",
    "showim(thd_img(inputim, thd=thd))\n",
    "\n",
    "im_transformed, Hinv_info = apply_random_H(inputim)\n",
    "im_orig = apply_H_from_info(im_transformed, Hinv_info)\n",
    "# im_orig = thd_img(im_orig, thd=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1636551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diff = thd_img(inputim, thd=thd)-thd_img(im_orig, thd=thd)\n",
    "showim(diff)\n",
    "showim(thd_img(im_orig, thd=thd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = thd_img(im_orig, thd=thd)\n",
    "torch.sum(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e9d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bfcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd776741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ef184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc611b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnp",
   "language": "python",
   "name": "pnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
