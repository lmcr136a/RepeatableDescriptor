{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efeb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import scipy.io as io\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import open3d as o3d\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils.utils import getWriterPath\n",
    "from settings import EXPER_PATH\n",
    "from utils.loader import dataLoader, modelLoader, pretrainedLoader\n",
    "from utils.logging import *\n",
    "from copy import deepcopy as dc\n",
    "from utils.d2s import DepthToSpace, SpaceToDepth\n",
    "from train_cubemap import *\n",
    "from utils.utils import flattenDetection\n",
    "from Train_model_frontend_cubemap import thd_img\n",
    "from utils_custom import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "def measure_t(start, before):\n",
    "    now = time.time()\n",
    "    runt = now-start\n",
    "    h = runt//3600\n",
    "    m = (runt - h*3600)//60\n",
    "    s = round(runt - h*3600 - m*60)\n",
    "    prtstr = f\"iter {_iter+1} in {len(train_loader)}, \\\n",
    "            {round((_iter+1)/len(train_loader), 4) * 100}%\\\n",
    "            {h}h {m}m {s}s\"\n",
    "    runt2 = before-start\n",
    "    h2 = runt2//3600\n",
    "    m2 = (runt2 - h2*3600)//60\n",
    "    s2 = round(runt2 - h2*3600 - m2*60)\n",
    "    prtstr += f\"  ({h2}h {m2}m {s2}s)\"\n",
    "    return now  \n",
    "args = Namespace(command='train_joint', config='configs/magicpoint_cubemap.yaml', debug=False, eval=False, exper_name='cubemap_dataset', func=train_joint)\n",
    "\n",
    "\n",
    "from utils_custom import *\n",
    "from utils_custom_visualize import *\n",
    "\n",
    "with open(args.config, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "EXPER_PATH = 'logs'\n",
    "output_dir = EXPER_PATH\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c018a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 09:34:42 koala-A520M-H root[4012480] INFO train on device: cuda\n",
      "2023-05-04 09:34:42 koala-A520M-H root[4012480] INFO => will save everything to logs/checkpoints\n",
      "2023-05-04 09:34:42 koala-A520M-H root[4012480] INFO workers_train: 1, workers_val: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: CubemapDataset\n",
      "not_warped_images:  False\n",
      "not_warped_images:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 09:34:43 koala-A520M-H root[4012480] INFO == train split size 64800 in 64800 batches\n",
      "2023-05-04 09:34:43 koala-A520M-H root[4012480] INFO == val split size 64800 in 64800 batches\n",
      "2023-05-04 09:34:43 koala-A520M-H root[4012480] INFO => creating model: SuperPointNet_cubemap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMONUM: 100   HOMOBATCH: 1\n",
      "use sparse_loss!\n",
      "set train loader\n",
      "set train loader\n",
      "model:  SuperPointNet_cubemap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 09:34:44 koala-A520M-H root[4012480] INFO => setting adam solver\n",
      "2023-05-04 09:34:44 koala-A520M-H root[4012480] INFO reset iterations to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from: %s logs/iter333_ce/checkpoints/RD_230504_091857_iter334_checkpoint.pth.tar\n",
      "successfully load pretrained model from: %s logs/iter333_ce/checkpoints/RD_230504_091857_iter334_checkpoint.pth.tar\n",
      "\n",
      "\n",
      " Train only descriptor\n"
     ]
    }
   ],
   "source": [
    "############################################### train_joint\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "task = config['data']['dataset']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info('train on device: %s', device)\n",
    "with open(os.path.join(output_dir, 'config.yml'), 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "# writer = SummaryWriter(getWriterPath(task=args.command, date=True))\n",
    "writer = SummaryWriter(getWriterPath(task=args.command, \n",
    "    exper_name=args.exper_name, date=True))\n",
    "## save data\n",
    "save_path = get_save_path(output_dir)\n",
    "\n",
    "############################################### \n",
    "data = dataLoader(config, dataset=task, warp_input=True)\n",
    "train_loader, val_loader = data['train_loader'], data['val_loader']\n",
    "\n",
    "datasize(train_loader, config, tag='train')\n",
    "datasize(val_loader, config, tag='val')\n",
    "# init the training agent using config file\n",
    "# from train_model_frontend import Train_model_frontend\n",
    "from utils.loader import get_module\n",
    "train_model_frontend = get_module('', config['front_end_model'])\n",
    "train_agent = train_model_frontend(config, save_path=save_path, device=device)\n",
    "\n",
    "# writer from tensorboard\n",
    "train_agent.writer = writer\n",
    "\n",
    "# feed the data into the agent\n",
    "train_agent.train_loader = train_loader\n",
    "train_agent.val_loader = val_loader\n",
    "\n",
    "# load model initiates the model and load the pretrained model (if any)\n",
    "train_agent.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074f9335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'warped_image', 'ply_path', 'R', 'T', 'R_w', 'T_w', 'img_path', 'img_path_w', 'kpts2D', 'kpts3D', 'kpts2D_w', 'kpts3D_w']) torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "sample =next(iter(train_loader))\n",
    "net = dc(train_agent.net)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1, betas=(0.9, 0.999))\n",
    "print(sample.keys(), sample['image'].shape)\n",
    "img, img_w = sample['image'].to(device), sample['warped_image'].to(device)\n",
    "batch_size, H, W = img.shape[0], img.shape[2], img.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a15b108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['image', 'warped_image', 'ply_path', 'R', 'T', 'R_w', 'T_w', 'img_path', 'img_path_w', 'kpts2D', 'kpts3D', 'kpts2D_w', 'kpts3D_w']),\n",
       " torch.Size([1, 3, 1024, 1024]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys(), sample['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da62b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "thd = 0.05\n",
    "B, h, b, loss = 0, 0, 0, 0\n",
    "\n",
    "bh = 0\n",
    "n_iter = 1  ##########\n",
    "\n",
    "\n",
    "dbk2d, dbk2d_w, dbk3d, dbk3d_w = sample['kpts2D'][B],sample['kpts2D_w'][B],sample['kpts3D'][B],sample['kpts3D_w'][B]        \n",
    "\n",
    "H_NUM_THIS = 2\n",
    "\n",
    "im_trf_cat, Hinv_infos = get_homo_img_cat(img, H_NUM_THIS)\n",
    "im_trf_cat_w, Hinv_infos_w = get_homo_img_cat(img_w, H_NUM_THIS)\n",
    "\n",
    "outs, outs_warp = (\n",
    "    net(im_trf_cat[B]),  # B는 배치를 뜻함, 배치는 1로 고정, (b, bh, c, H, W)\n",
    "    net(im_trf_cat_w[B]),# 모델 인풋은 4차원이어야 하므로 bh를 b처럼 이용\n",
    "    )\n",
    "semi, coarse_desc = outs['semi'], outs['desc']\n",
    "hms = flattenDetection(semi)\n",
    "semi_w, coarse_desc_w = outs_warp['semi'], outs_warp['desc']\n",
    "hms_w = flattenDetection(semi_w)\n",
    "\n",
    "desc = train_agent.interpolate_to_dense(coarse_desc)[bh]\n",
    "desc_w = train_agent.interpolate_to_dense(coarse_desc_w)[bh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba66021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1024, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7711bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koala/anaconda3/envs/pnp/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from utils.loss_functions.custom_loss import *\n",
    "\n",
    "imgshape = (H_NUM_THIS, H, W)\n",
    "labels3D_in_loss = getLabels(imgshape, dbk2d, device=device)\n",
    "labels3D_in_loss_w = getLabels(imgshape, dbk2d, device=device)\n",
    "\n",
    "from copy import deepcopy as dc\n",
    "############################################################################\n",
    "mask_3D_flattened, mask2D = getMasks(imgshape, dc(Hinv_infos[bh]), device=device)\n",
    "loss_det = detection_loss_custom(\n",
    "    semi[bh], labels3D_in_loss[bh], mask_3D_flattened, device=device\n",
    ")\n",
    "\n",
    "mask_3D_flattened_w, mask2D_w = getMasks(imgshape, dc(Hinv_infos_w[bh]), device=device)\n",
    "loss_det_w = detection_loss_custom(\n",
    "    semi_w[bh], labels3D_in_loss_w[bh], mask_3D_flattened_w, device=device\n",
    ")\n",
    "############################################################################\n",
    "\n",
    "hm = apply_H_from_info(hms[bh], Hinv_infos[bh])\n",
    "hm = thd_img(hm, thd=thd)\n",
    "kpts = get_kpts_from_hm(hm, mask2D)\n",
    "\n",
    "hm_w = apply_H_from_info(hms_w[bh], Hinv_infos_w[bh])\n",
    "hm_w = thd_img(hm_w, thd=thd)\n",
    "kpts_w = get_kpts_from_hm(hm_w, mask2D_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3daf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_kpts_idx, matched_kpts_idx_w = get_matches(\n",
    "    dbk2d, dbk3d.tolist(), dbk2d_w, dbk3d_w.tolist(),\n",
    "    kpts, kpts_w,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts_desc = get_desc_of_kpts(desc, kpts)\n",
    "kpts_desc_w = get_desc_of_kpts(desc_w, kpts_w)\n",
    "kpts_desc = kpts_desc.transpose(0, 1)\n",
    "kpts_desc_w = kpts_desc_w.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9465bf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, torch.Size([1987, 256]), 3797, torch.Size([3797, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kpts), kpts_desc.shape, len(kpts_w), kpts_desc_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f99230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1987, 2]) torch.Size([1987, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kpts = torch.Tensor(kpts).to(device)\n",
    "kpts_w = torch.Tensor(kpts_w).to(device)\n",
    "print(kpts.shape, kpts_desc.shape)\n",
    "plt.figure()\n",
    "\n",
    "matched_kpts_idx, matched_kpts_idx_w = torch.Tensor(matched_kpts_idx), torch.Tensor(matched_kpts_idx_w)\n",
    "maching_plot(img, img_w, kpts, kpts_w, \n",
    "    kpts[matched_kpts_idx.int()], kpts_w[matched_kpts_idx_w.int()], \n",
    "    path=os.path.join(f'test_training_this.png'))\n",
    "plt.close()                \n",
    "kpts_sorted, kpts_w_sorted, color = sort_by_similar_pts(kpts, kpts_w, kpts_desc, kpts_desc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73287ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpts_desc = get_desc_of_kpts(desc, kpts)\n",
    "kpts_desc_w = get_desc_of_kpts(desc_w, kpts_w)\n",
    "kpts_desc = kpts_desc.transpose(0, 1).detach().cpu()\n",
    "kpts_desc_w = kpts_desc_w.transpose(0, 1).detach().cpu()\n",
    "kpts_sorted, kpts_w_sorted, color = sort_by_similar_pts(kpts.int(), kpts_w.int(), kpts_desc, kpts_desc_w)\n",
    "plt.figure()\n",
    "maching_plot(img, img_w, kpts, kpts_w, \n",
    "    kpts_sorted.detach().cpu(), kpts_w_sorted.detach().cpu(), color,\n",
    "    path=os.path.join(f'test_predicted_this.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70710ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09afe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff658d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c01c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m outfig \u001b[38;5;241m=\u001b[39m maching_plot(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path_w\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m      2\u001b[0m         kpts, kpts_w, \n\u001b[0;32m----> 3\u001b[0m         \u001b[43mkpts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmatched_kpts_idx\u001b[49m\u001b[43m]\u001b[49m, kpts_w[matched_kpts_idx_w],\n\u001b[1;32m      4\u001b[0m                                  img_shape\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(outfig)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrd_glue0.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "outfig = maching_plot(sample['img_path'][0], sample['img_path_w'][0], \n",
    "        kpts, kpts_w, \n",
    "        kpts[matched_kpts_idx], kpts_w[matched_kpts_idx_w],\n",
    "                                 img_shape=img.shape[2:])\n",
    "plt.imshow(outfig)\n",
    "plt.savefig('rd_glue0.jpg', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matched_kpts_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bfcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080865d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# a = torch.Tensor([[1,1],[2,2],[3,3],[0,0],[0,0], [5,0]])\n",
    "idx = torch.where(a[:,1]+a[:,0] != 0)[0]\n",
    "a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a026587",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,1],[2,2],[3,3],[0,0],[0,0], [5,0]])\n",
    "idx = np.where(a[:,1]+a[:,0] != 0)[0]\n",
    "a[idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cb3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8feef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85073673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnp",
   "language": "python",
   "name": "pnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
